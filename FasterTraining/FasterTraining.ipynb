{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import time\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from watermark import watermark\n",
    "\n",
    "from local_dataset_utilities import (\n",
    "    download_dataset,\n",
    "    load_dataset_into_to_dataframe,\n",
    "    partition_dataset,\n",
    ")\n",
    "from local_dataset_utilities import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.9.0\n",
      "\n",
      "torch       : 2.0.0\n",
      "lightning   : 2.0.1\n",
      "transformers: 4.27.4\n",
      "\n",
      "Torch CUDA available? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:38<00:00, 1287.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3684af0454ce4c22abf02f8d7a77af1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n",
      "Tokenizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7b1340d1a867811b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab452415747842a0afc2dea3f926dd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-49742e050263380f.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001/0003 | Batch 0000/2916 | Loss: 0.6944\n",
      "Epoch: 0001/0003 | Batch 0300/2916 | Loss: 0.3054\n",
      "Epoch: 0001/0003 | Batch 0600/2916 | Loss: 0.3536\n",
      "Epoch: 0001/0003 | Batch 0900/2916 | Loss: 0.2029\n",
      "Epoch: 0001/0003 | Batch 1200/2916 | Loss: 0.2000\n",
      "Epoch: 0001/0003 | Batch 1500/2916 | Loss: 0.5751\n",
      "Epoch: 0001/0003 | Batch 1800/2916 | Loss: 0.4488\n",
      "Epoch: 0001/0003 | Batch 2100/2916 | Loss: 0.7184\n",
      "Epoch: 0001/0003 | Batch 2400/2916 | Loss: 0.2910\n",
      "Epoch: 0001/0003 | Batch 2700/2916 | Loss: 0.2338\n",
      "Epoch: 0001/0003 | Train acc.: 89.79% | Val acc.: 92.57%\n",
      "Epoch: 0002/0003 | Batch 0000/2916 | Loss: 0.1279\n",
      "Epoch: 0002/0003 | Batch 0300/2916 | Loss: 0.0119\n",
      "Epoch: 0002/0003 | Batch 0600/2916 | Loss: 0.0241\n",
      "Epoch: 0002/0003 | Batch 0900/2916 | Loss: 0.1540\n",
      "Epoch: 0002/0003 | Batch 1200/2916 | Loss: 0.0069\n",
      "Epoch: 0002/0003 | Batch 1500/2916 | Loss: 0.0144\n",
      "Epoch: 0002/0003 | Batch 1800/2916 | Loss: 0.0516\n",
      "Epoch: 0002/0003 | Batch 2100/2916 | Loss: 0.0403\n",
      "Epoch: 0002/0003 | Batch 2400/2916 | Loss: 0.1503\n",
      "Epoch: 0002/0003 | Batch 2700/2916 | Loss: 0.0153\n",
      "Epoch: 0002/0003 | Train acc.: 95.22% | Val acc.: 91.61%\n",
      "Epoch: 0003/0003 | Batch 0000/2916 | Loss: 0.0168\n",
      "Epoch: 0003/0003 | Batch 0300/2916 | Loss: 0.1900\n",
      "Epoch: 0003/0003 | Batch 0600/2916 | Loss: 0.0068\n",
      "Epoch: 0003/0003 | Batch 0900/2916 | Loss: 0.0674\n",
      "Epoch: 0003/0003 | Batch 1200/2916 | Loss: 0.1615\n",
      "Epoch: 0003/0003 | Batch 1500/2916 | Loss: 0.0040\n",
      "Epoch: 0003/0003 | Batch 1800/2916 | Loss: 0.0165\n",
      "Epoch: 0003/0003 | Batch 2100/2916 | Loss: 0.0070\n",
      "Epoch: 0003/0003 | Batch 2400/2916 | Loss: 0.0899\n",
      "Epoch: 0003/0003 | Batch 2700/2916 | Loss: 0.0556\n",
      "Epoch: 0003/0003 | Train acc.: 97.45% | Val acc.: 92.65%\n",
      "Time elapsed 85.65 min\n",
      "Test accuracy 92.44%\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs, model, optimizer, train_loader, val_loader, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            model.train()\n",
    "            for s in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "                batch[s] = batch[s].to(device)\n",
    "\n",
    "            ### FORWARD AND BACK PROP\n",
    "            outputs = model(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            outputs[\"loss\"].backward()\n",
    "\n",
    "            ### UPDATE MODEL PARAMETERS\n",
    "            optimizer.step()\n",
    "\n",
    "            ### LOGGING\n",
    "            if not batch_idx % 300:\n",
    "                print(\n",
    "                    f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {outputs['loss']:.4f}\"\n",
    "                )\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
    "                train_acc.update(predicted_labels, batch[\"label\"])\n",
    "\n",
    "        ### MORE LOGGING\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "            for batch in val_loader:\n",
    "                for s in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "                    batch[s] = batch[s].to(device)\n",
    "                outputs = model(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    labels=batch[\"label\"],\n",
    "                )\n",
    "                predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
    "                val_acc.update(predicted_labels, batch[\"label\"])\n",
    "\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Train acc.: {train_acc.compute()*100:.2f}% | Val acc.: {val_acc.compute()*100:.2f}%\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(watermark(packages=\"torch,lightning,transformers\", python=True))\n",
    "    print(\"Torch CUDA available?\", torch.cuda.is_available())\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    ##########################\n",
    "    ### 1 Loading the Dataset\n",
    "    ##########################\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    if not (op.exists(\"train.csv\") and op.exists(\"val.csv\") and op.exists(\"test.csv\")):\n",
    "        partition_dataset(df)\n",
    "\n",
    "    imdb_dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"train.csv\",\n",
    "            \"validation\": \"val.csv\",\n",
    "            \"test\": \"test.csv\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 2 Tokenization and Numericalization\n",
    "    #########################################\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"Tokenizer input max length:\", tokenizer.model_max_length, flush=True)\n",
    "    print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size, flush=True)\n",
    "\n",
    "    print(\"Tokenizing ...\", flush=True)\n",
    "    imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
    "    del imdb_dataset\n",
    "    imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    #########################################\n",
    "    ### 3 Set Up DataLoaders\n",
    "    #########################################\n",
    "\n",
    "    train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "    val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "    test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 4 Initializing the Model\n",
    "    #########################################\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=2\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "    #########################################\n",
    "    ### 5 Finetuning\n",
    "    #########################################\n",
    "\n",
    "    start = time.time()\n",
    "    train(\n",
    "        num_epochs=3,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2).to(device)\n",
    "        for batch in test_loader:\n",
    "            for s in [\"input_ids\", \"attention_mask\", \"label\"]:\n",
    "                batch[s] = batch[s].to(device)\n",
    "            outputs = model(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "            predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
    "            test_acc.update(predicted_labels, batch[\"label\"])\n",
    "\n",
    "    print(f\"Test accuracy {test_acc.compute()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as op\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from watermark import watermark\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from local_dataset_utilities import (\n",
    "    download_dataset,\n",
    "    load_dataset_into_to_dataframe,\n",
    "    partition_dataset,\n",
    "    IMDBDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "        self.log(\"train_loss\", outputs[\"loss\"])\n",
    "        with torch.no_grad():\n",
    "            logits = outputs[\"logits\"]\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            self.train_acc(predicted_labels, batch[\"label\"])\n",
    "            self.log(\"train_acc\", self.train_acc, on_epoch=True, on_step=False)\n",
    "        return outputs[\"loss\"]  # this is passed to the optimizer for training\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.val_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.test_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"accuracy\", self.test_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.trainer.model.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.9.0\n",
      "\n",
      "torch       : 2.0.0\n",
      "lightning   : 2.0.1\n",
      "transformers: 4.27.4\n",
      "\n",
      "Torch CUDA available? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:44<00:00, 1134.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6617b5a8d0224d99bf87137cb559097f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n",
      "Tokenizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c2f5742bf1667880.arrow\n",
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-874b2c0df8c65d25.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383aa341987f46da8a2ef60fa0a995cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/my-model\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                                | Params\n",
      "------------------------------------------------------------------\n",
      "0 | model     | DistilBertForSequenceClassification | 67.0 M\n",
      "1 | train_acc | MulticlassAccuracy                  | 0     \n",
      "2 | val_acc   | MulticlassAccuracy                  | 0     \n",
      "3 | test_acc  | MulticlassAccuracy                  | 0     \n",
      "------------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b999c6d48f5548689d9751fbd689236b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbefd7562f7b43a7a6af50b983dc93da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de28604600e482893787da67ac2dd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed5ea15d8464ba79432091599d319a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ff7544f6d84a799332496c0d5ce8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=1-step=5832.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 128.02 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=1-step=5832.ckpt\n",
      "/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eceea12cfa2245f088cb78c2ffb196bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9209684133529663    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n",
      "[{'accuracy': 0.9209684133529663}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(watermark(packages=\"torch,lightning,transformers\", python=True), flush=True)\n",
    "    print(\"Torch CUDA available?\", torch.cuda.is_available(), flush=True)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "\n",
    "    ##########################\n",
    "    ### 1 Loading the Dataset\n",
    "    ##########################\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    if not (op.exists(\"train.csv\") and op.exists(\"val.csv\") and op.exists(\"test.csv\")):\n",
    "        partition_dataset(df)\n",
    "\n",
    "    imdb_dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"train.csv\",\n",
    "            \"validation\": \"val.csv\",\n",
    "            \"test\": \"test.csv\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 2 Tokenization and Numericalization\n",
    "    ########################################\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"Tokenizer input max length:\", tokenizer.model_max_length, flush=True)\n",
    "    print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size, flush=True)\n",
    "\n",
    "    print(\"Tokenizing ...\", flush=True)\n",
    "    imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
    "    del imdb_dataset\n",
    "    imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    #########################################\n",
    "    ### 3 Set Up DataLoaders\n",
    "    #########################################\n",
    "\n",
    "    train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "    val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "    test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 4 Initializing the Model\n",
    "    #########################################\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=2\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 5 Finetuning\n",
    "    #########################################\n",
    "\n",
    "    lightning_model = LightningModel(model)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\")  # save top 1 model\n",
    "    ]\n",
    "    logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=3,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    trainer.fit(\n",
    "        model=lightning_model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
    "\n",
    "    test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")\n",
    "    print(test_acc)\n",
    "\n",
    "    with open(op.join(trainer.logger.log_dir, \"outputs.txt\"), \"w\") as f:\n",
    "        f.write((f\"Time elapsed {elapsed/60:.2f} min\\n\"))\n",
    "        f.write(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36c88db02df3045e303d5bda7cc7a68efeb5214396e6ab408a715392673add6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
