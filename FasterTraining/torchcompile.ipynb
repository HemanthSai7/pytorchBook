{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import os.path as op\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from watermark import watermark\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from local_dataset_utilities import (\n",
    "    download_dataset,\n",
    "    load_dataset_into_to_dataframe,\n",
    "    partition_dataset,\n",
    "    IMDBDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(L.LightningModule):\n",
    "    def __init__(self, model, learning_rate=5e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "        self.log(\"train_loss\", outputs[\"loss\"])\n",
    "        with torch.no_grad():\n",
    "            logits = outputs[\"logits\"]\n",
    "            predicted_labels = torch.argmax(logits, 1)\n",
    "            self.train_acc(predicted_labels, batch[\"label\"])\n",
    "            self.log(\"train_acc\", self.train_acc, on_epoch=True, on_step=False)\n",
    "        return outputs[\"loss\"]  # this is passed to the optimizer for training\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "        self.log(\"val_loss\", outputs[\"loss\"], prog_bar=True)\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.val_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"],\n",
    "        )\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        predicted_labels = torch.argmax(logits, 1)\n",
    "        self.test_acc(predicted_labels, batch[\"label\"])\n",
    "        self.log(\"accuracy\", self.test_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.trainer.model.parameters(), lr=self.learning_rate\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.9.0\n",
      "\n",
      "torch       : 2.0.0\n",
      "lightning   : 2.0.1\n",
      "transformers: 4.27.4\n",
      "\n",
      "Torch CUDA available? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:43<00:00, 1141.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462b7f260b464ab38d254682cff6819c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n",
      "Tokenizing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-c2f5742bf1667880.arrow\n",
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-874b2c0df8c65d25.arrow\n",
      "Loading cached processed dataset at /home/sai/.cache/huggingface/datasets/csv/default-4e0c018788380fce/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0e05a8fea06ed97d.arrow\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory logs/my-model/version_0/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | OptimizedModule    | 67.0 M\n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:188: UserWarning: Experiment logs directory logs/my-model/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f50c3dfb0c0487986a1fd65e0ed2714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5b3e7d6d4e42078d5a26d1ed62bdce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-04-03 16:32:07,776] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-04-03 16:32:17,124] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac23e1481bb44f3b7ee3e8e4d08368e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18bc348c4a344da7b56ccda262c63816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d075ccfa7c72460897fd6b7c86548771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=0-step=2916-v1.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 28.22 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=0-step=2916-v1.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ec56d7d4564e29b7a94f95f7e8b111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to collect metadata on function, produced code may be suboptimal.  Known situations this can occur are inference mode only compilation involving resize_ or prims (!schema.hasAnyAliasInfo() INTERNAL ASSERT FAILED); if your situation looks different please file a bug to PyTorch.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 1674, in aot_wrapper_dedupe\n",
      "    fw_metadata, _out = run_functionalized_fw_and_collect_metadata(\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 606, in inner\n",
      "    flat_f_outs = f(*flat_f_args)\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py\", line 2776, in functional_call\n",
      "    out = Interpreter(mod).run(*args[params_len:], **kwargs)\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py\", line 136, in run\n",
      "    self.env[node] = self.run_node(node)\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py\", line 177, in run_node\n",
      "    return getattr(self, n.op)(n.target, args, kwargs)\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py\", line 249, in call_function\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_inductor/overrides.py\", line 38, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "RuntimeError: Inference tensors do not track version counter.\n",
      "\n",
      "While executing %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_distilbert_embeddings_position_ids, (slice(None, None, None), slice(None, 512, None))), kwargs = {})\n",
      "Original traceback:\n",
      "  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 128, in forward\n",
      "    position_ids = self.position_ids[:, :seq_length]\n",
      " |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 581, in forward\n",
      "    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n",
      " |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 763, in forward\n",
      "    distilbert_output = self.distilbert(\n",
      "\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "debug_wrapper raised RuntimeError: Inference tensors do not track version counter.\n\nWhile executing %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_distilbert_embeddings_position_ids, (slice(None, None, None), slice(None, 512, None))), kwargs = {})\nOriginal traceback:\n  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 128, in forward\n    position_ids = self.position_ids[:, :seq_length]\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 581, in forward\n    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 763, in forward\n    distilbert_output = self.distilbert(\n\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:670\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 670\u001b[0m     compiled_fn \u001b[39m=\u001b[39m compiler_fn(gm, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfake_example_inputs())\n\u001b[1;32m    671\u001b[0m _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdone compiler function \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/debug_utils.py:1055\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m     compiled_gm \u001b[39m=\u001b[39m compiler_fn(gm, example_inputs)\n\u001b[1;32m   1057\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/__init__.py:1390\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1388\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_inductor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompile_fx\u001b[39;00m \u001b[39mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1390\u001b[0m \u001b[39mreturn\u001b[39;00m compile_fx(model_, inputs_, config_patches\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:455\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mwith\u001b[39;00m overrides\u001b[39m.\u001b[39mpatch_functions():\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m     \u001b[39m# TODO: can add logging before/after the call to create_aot_dispatcher_function\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[39m# in torch._functorch/aot_autograd.py::aot_module_simplified::aot_function_simplified::new_func\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[39m# once torchdynamo is merged into pytorch\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m aot_autograd(\n\u001b[1;32m    456\u001b[0m         fw_compiler\u001b[39m=\u001b[39;49mfw_compiler,\n\u001b[1;32m    457\u001b[0m         bw_compiler\u001b[39m=\u001b[39;49mbw_compiler,\n\u001b[1;32m    458\u001b[0m         decompositions\u001b[39m=\u001b[39;49mselect_decomp_table(),\n\u001b[1;32m    459\u001b[0m         partition_fn\u001b[39m=\u001b[39;49mfunctools\u001b[39m.\u001b[39;49mpartial(\n\u001b[1;32m    460\u001b[0m             min_cut_rematerialization_partition, compiler\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minductor\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    461\u001b[0m         ),\n\u001b[1;32m    462\u001b[0m         keep_inference_input_mutations\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    463\u001b[0m     )(model_, example_inputs_)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:48\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mwith\u001b[39;00m enable_aot_logging():\n\u001b[0;32m---> 48\u001b[0m     cg \u001b[39m=\u001b[39m aot_module_simplified(gm, example_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     49\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39maot_autograd\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2805\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, hasher_type, static_argnums, keep_inference_input_mutations)\u001b[0m\n\u001b[1;32m   2803\u001b[0m full_args\u001b[39m.\u001b[39mextend(args)\n\u001b[0;32m-> 2805\u001b[0m compiled_fn \u001b[39m=\u001b[39m create_aot_dispatcher_function(\n\u001b[1;32m   2806\u001b[0m     functional_call,\n\u001b[1;32m   2807\u001b[0m     full_args,\n\u001b[1;32m   2808\u001b[0m     aot_config,\n\u001b[1;32m   2809\u001b[0m )\n\u001b[1;32m   2811\u001b[0m \u001b[39m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m   2812\u001b[0m \u001b[39m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m   2813\u001b[0m \u001b[39m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m   2814\u001b[0m \u001b[39m# convention.  This should get fixed...\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2498\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   2496\u001b[0m \u001b[39m# You can put more passes here\u001b[39;00m\n\u001b[0;32m-> 2498\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(flat_fn, fake_flat_args, aot_config)\n\u001b[1;32m   2500\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1802\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn)\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_fn(\u001b[39m*\u001b[39madd_dupe_args(args))\n\u001b[0;32m-> 1802\u001b[0m compiled_fn \u001b[39m=\u001b[39m compiler_fn(wrapped_flat_fn, deduped_flat_args, aot_config)\n\u001b[1;32m   1804\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(compiled_fn, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1278\u001b[0m, in \u001b[0;36maot_dispatch_base\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m   1277\u001b[0m \u001b[39mwith\u001b[39;00m enable_python_dispatcher():\n\u001b[0;32m-> 1278\u001b[0m     _fw_metadata, _out \u001b[39m=\u001b[39m run_functionalized_fw_and_collect_metadata(\n\u001b[1;32m   1279\u001b[0m         flat_fn,\n\u001b[1;32m   1280\u001b[0m         keep_input_mutations\u001b[39m=\u001b[39;49maot_config\u001b[39m.\u001b[39;49mkeep_inference_input_mutations,\n\u001b[1;32m   1281\u001b[0m     )(\n\u001b[1;32m   1282\u001b[0m         \u001b[39m*\u001b[39;49mflat_args\n\u001b[1;32m   1283\u001b[0m     )\n\u001b[1;32m   1285\u001b[0m _input_info \u001b[39m=\u001b[39m _fw_metadata\u001b[39m.\u001b[39minput_info\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:606\u001b[0m, in \u001b[0;36mrun_functionalized_fw_and_collect_metadata.<locals>.inner\u001b[0;34m(*flat_args)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    605\u001b[0m     \u001b[39m# precondition: The passed in function already handles unflattening inputs + flattening outputs\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m     flat_f_outs \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mflat_f_args)\n\u001b[1;32m    607\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1800\u001b[0m, in \u001b[0;36maot_wrapper_dedupe.<locals>.wrapped_flat_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1798\u001b[0m \u001b[39m@wraps\u001b[39m(flat_fn)\n\u001b[1;32m   1799\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_flat_fn\u001b[39m(\u001b[39m*\u001b[39margs):\n\u001b[0;32m-> 1800\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_fn(\u001b[39m*\u001b[39;49madd_dupe_args(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2776\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.functional_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mdetect_anomaly(check_nan\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m-> 2776\u001b[0m             out \u001b[39m=\u001b[39m Interpreter(mod)\u001b[39m.\u001b[39;49mrun(\u001b[39m*\u001b[39;49margs[params_len:], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2777\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py:136\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv[node] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_node(node)\n\u001b[1;32m    137\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py:177\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(kwargs, \u001b[39mdict\u001b[39m)\n\u001b[0;32m--> 177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, n\u001b[39m.\u001b[39;49mop)(n\u001b[39m.\u001b[39;49mtarget, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/fx/interpreter.py:249\u001b[0m, in \u001b[0;36mInterpreter.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39m# Execute the function and return the result\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m \u001b[39mreturn\u001b[39;00m target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_inductor/overrides.py:38\u001b[0m, in \u001b[0;36mAutogradMonkeypatch.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m replacements[func](\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Inference tensors do not track version counter.\n\nWhile executing %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_distilbert_embeddings_position_ids, (slice(None, None, None), slice(None, 512, None))), kwargs = {})\nOriginal traceback:\n  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 128, in forward\n    position_ids = self.position_ids[:, :seq_length]\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 581, in forward\n    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 763, in forward\n    distilbert_output = self.distilbert(\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m elapsed \u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m start\n\u001b[1;32m    108\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime elapsed \u001b[39m\u001b[39m{\u001b[39;00melapsed\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m test_acc \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtest(lightning_model, dataloaders\u001b[39m=\u001b[39;49mtest_loader, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    111\u001b[0m \u001b[39mprint\u001b[39m(test_acc)\n\u001b[1;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(op\u001b[39m.\u001b[39mjoin(trainer\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mlog_dir, \u001b[39m\"\u001b[39m\u001b[39moutputs.txt\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:706\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    704\u001b[0m     model \u001b[39m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    705\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 706\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    707\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    708\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     43\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 44\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:749\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(model, test_dataloaders\u001b[39m=\u001b[39mdataloaders, datamodule\u001b[39m=\u001b[39mdatamodule)\n\u001b[1;32m    746\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    747\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn, ckpt_path, model_provided\u001b[39m=\u001b[39mmodel_provided, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    748\u001b[0m )\n\u001b[0;32m--> 749\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    750\u001b[0m \u001b[39m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    751\u001b[0m results \u001b[39m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:935\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    932\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 935\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    937\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    940\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:971\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mbarrier(\u001b[39m\"\u001b[39m\u001b[39mrun-stage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    970\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    972\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m    973\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_loop\u001b[39m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[39m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx)\n\u001b[1;32m    116\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_kwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:288\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 288\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    290\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    291\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:387\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[1;32m    386\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mLightningModel.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 43\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m     44\u001b[0m         batch[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     45\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     46\u001b[0m         labels\u001b[39m=\u001b[39;49mbatch[\u001b[39m\"\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     logits \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m     predicted_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(logits, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mLightningModel.forward\u001b[0;34m(self, input_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, labels):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:337\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_size)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[39mreturn\u001b[39;00m hijacked_callback(frame, cache_size, hooks)\n\u001b[1;32m    336\u001b[0m \u001b[39mwith\u001b[39;00m compile_lock:\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mreturn\u001b[39;00m callback(frame, cache_size, hooks)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:404\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    402\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    403\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 404\u001b[0m     result \u001b[39m=\u001b[39m inner_convert(frame, cache_size, hooks)\n\u001b[1;32m    405\u001b[0m     counters[\u001b[39m\"\u001b[39m\u001b[39mframes\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mok\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:104\u001b[0m, in \u001b[0;36mwrap_convert_context.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m torch\u001b[39m.\u001b[39mfx\u001b[39m.\u001b[39mgraph_module\u001b[39m.\u001b[39m_forward_from_src \u001b[39m=\u001b[39m fx_forward_from_src_skip_result\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    105\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_grad_enabled(prior_grad_mode)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:262\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_size, hooks)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mglobal\u001b[39;00m initial_grad_state\n\u001b[1;32m    260\u001b[0m initial_grad_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m _compile(\n\u001b[1;32m    263\u001b[0m     frame\u001b[39m.\u001b[39;49mf_code,\n\u001b[1;32m    264\u001b[0m     frame\u001b[39m.\u001b[39;49mf_globals,\n\u001b[1;32m    265\u001b[0m     frame\u001b[39m.\u001b[39;49mf_locals,\n\u001b[1;32m    266\u001b[0m     frame\u001b[39m.\u001b[39;49mf_builtins,\n\u001b[1;32m    267\u001b[0m     compiler_fn,\n\u001b[1;32m    268\u001b[0m     one_graph,\n\u001b[1;32m    269\u001b[0m     export,\n\u001b[1;32m    270\u001b[0m     hooks,\n\u001b[1;32m    271\u001b[0m     frame,\n\u001b[1;32m    272\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:324\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, hooks, frame)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39mfor\u001b[39;00m attempt \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m    323\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m         out_code \u001b[39m=\u001b[39m transform_code_object(code, transform)\n\u001b[1;32m    325\u001b[0m         orig_code_map[out_code] \u001b[39m=\u001b[39m code\n\u001b[1;32m    326\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:445\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m    442\u001b[0m instructions \u001b[39m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m    443\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m--> 445\u001b[0m transformations(instructions, code_options)\n\u001b[1;32m    446\u001b[0m \u001b[39mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:311\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mnonlocal\u001b[39;00m output\n\u001b[1;32m    299\u001b[0m tracer \u001b[39m=\u001b[39m InstructionTranslator(\n\u001b[1;32m    300\u001b[0m     instructions,\n\u001b[1;32m    301\u001b[0m     code,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     mutated_closure_cell_contents,\n\u001b[1;32m    310\u001b[0m )\n\u001b[0;32m--> 311\u001b[0m tracer\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    312\u001b[0m output \u001b[39m=\u001b[39m tracer\u001b[39m.\u001b[39moutput\n\u001b[1;32m    313\u001b[0m \u001b[39massert\u001b[39;00m output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1726\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1725\u001b[0m     _step_logger()(logging\u001b[39m.\u001b[39mINFO, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorchdynamo start tracing \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_code\u001b[39m.\u001b[39mco_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1726\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:576\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mpush_tx(\u001b[39mself\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m    574\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstruction_pointer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    575\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mshould_exit\n\u001b[0;32m--> 576\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:540\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, inst\u001b[39m.\u001b[39mopname):\n\u001b[1;32m    539\u001b[0m         unimplemented(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmissing: \u001b[39m\u001b[39m{\u001b[39;00minst\u001b[39m.\u001b[39mopname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, inst\u001b[39m.\u001b[39;49mopname)(inst)\n\u001b[1;32m    542\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\u001b[39m.\u001b[39mopname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRETURN_VALUE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:372\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    370\u001b[0m     reason \u001b[39m=\u001b[39m GraphCompileReason(excp\u001b[39m.\u001b[39mmsg, user_stack)\n\u001b[1;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrestore_graphstate(state)\n\u001b[0;32m--> 372\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput\u001b[39m.\u001b[39;49mcompile_subgraph(\u001b[39mself\u001b[39;49m, reason\u001b[39m=\u001b[39;49mreason)\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpopn(push \u001b[39m-\u001b[39m dis\u001b[39m.\u001b[39mstack_effect(inst\u001b[39m.\u001b[39mopcode, inst\u001b[39m.\u001b[39marg))\n\u001b[1;32m    375\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(push):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:541\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    538\u001b[0m output \u001b[39m=\u001b[39m []\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m count_calls(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    540\u001b[0m     output\u001b[39m.\u001b[39mextend(\n\u001b[0;32m--> 541\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompile_and_call_fx_graph(tx, pass2\u001b[39m.\u001b[39;49mgraph_output_vars(), root)\n\u001b[1;32m    542\u001b[0m     )\n\u001b[1;32m    544\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pass2\u001b[39m.\u001b[39mgraph_outputs) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    545\u001b[0m         output\u001b[39m.\u001b[39mappend(pass2\u001b[39m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:588\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m    586\u001b[0m assert_no_fake_params_or_buffers(gm)\n\u001b[1;32m    587\u001b[0m \u001b[39mwith\u001b[39;00m tracing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracing_context):\n\u001b[0;32m--> 588\u001b[0m     compiled_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_user_compiler(gm)\n\u001b[1;32m    589\u001b[0m compiled_fn \u001b[39m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m    591\u001b[0m counters[\u001b[39m\"\u001b[39m\u001b[39mstats\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39munique_graphs\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/utils.py:163\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     compilation_metrics[key] \u001b[39m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m t0 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 163\u001b[0m r \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    164\u001b[0m time_spent \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t0\n\u001b[1;32m    165\u001b[0m \u001b[39m# print(f\"Dynamo timer: key={key}, latency={latency:.2f} sec\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorchML/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:675\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    674\u001b[0m     compiled_fn \u001b[39m=\u001b[39m gm\u001b[39m.\u001b[39mforward\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m BackendCompilerFailed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiler_fn, e) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: debug_wrapper raised RuntimeError: Inference tensors do not track version counter.\n\nWhile executing %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_distilbert_embeddings_position_ids, (slice(None, None, None), slice(None, 512, None))), kwargs = {})\nOriginal traceback:\n  File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 128, in forward\n    position_ids = self.position_ids[:, :seq_length]\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 581, in forward\n    embeddings = self.embeddings(input_ids, inputs_embeds)  # (bs, seq_length, dim)\n |   File \"/home/sai/anaconda3/envs/pytorchML/lib/python3.10/site-packages/transformers/models/distilbert/modeling_distilbert.py\", line 763, in forward\n    distilbert_output = self.distilbert(\n\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(watermark(packages=\"torch,lightning,transformers\", python=True), flush=True)\n",
    "    print(\"Torch CUDA available?\", torch.cuda.is_available(), flush=True)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    ##########################\n",
    "    ### 1 Loading the Dataset\n",
    "    ##########################\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    if not (op.exists(\"train.csv\") and op.exists(\"val.csv\") and op.exists(\"test.csv\")):\n",
    "        partition_dataset(df)\n",
    "\n",
    "    imdb_dataset = load_dataset(\n",
    "        \"csv\",\n",
    "        data_files={\n",
    "            \"train\": \"train.csv\",\n",
    "            \"validation\": \"val.csv\",\n",
    "            \"test\": \"test.csv\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 2 Tokenization and Numericalization\n",
    "    ########################################\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    print(\"Tokenizer input max length:\", tokenizer.model_max_length, flush=True)\n",
    "    print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size, flush=True)\n",
    "\n",
    "    print(\"Tokenizing ...\", flush=True)\n",
    "    imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
    "    del imdb_dataset\n",
    "    imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "    #########################################\n",
    "    ### 3 Set Up DataLoaders\n",
    "    #########################################\n",
    "\n",
    "    train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "    val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "    test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=12,\n",
    "        shuffle=True,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=12,\n",
    "        num_workers=12,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 4 Initializing the Model\n",
    "    #########################################\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=2\n",
    "    )\n",
    "\n",
    "    #########################################\n",
    "    ### 5 Finetuning\n",
    "    #########################################\n",
    "\n",
    "    model=torch.compile(model)\n",
    "    lightning_model = LightningModel(model)\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(save_top_k=1, mode=\"max\", monitor=\"val_acc\")  # save top 1 model\n",
    "    ]\n",
    "    logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=3,\n",
    "        callbacks=callbacks,\n",
    "        accelerator=\"gpu\",\n",
    "        precision=\"16-mixed\",\n",
    "        devices=[0],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "        deterministic=True,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    trainer.fit(\n",
    "        model=lightning_model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=val_loader,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
    "\n",
    "    test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\")\n",
    "    print(test_acc)\n",
    "\n",
    "    with open(op.join(trainer.logger.log_dir, \"outputs.txt\"), \"w\") as f:\n",
    "        f.write((f\"Time elapsed {elapsed/60:.2f} min\\n\"))\n",
    "        f.write(f\"Test acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
